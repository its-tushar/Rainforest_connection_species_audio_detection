{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtUYoWzdc7o4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tnrange\n",
    "from sklearn.metrics import f1_score\n",
    "import librosa\n",
    "import pickle\n",
    "import ast\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense,Masking,Concatenate,GlobalAveragePooling1D,Dropout,Conv2D,TimeDistributed,Conv1D,BatchNormalization,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "%reload_ext tensorboard\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.family'] = 'Dejavu Sans'\n",
    "data=pd.read_csv('/content/train_tp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccBqvDexcwac"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFQ3ggRnc1OX"
   },
   "source": [
    "First, we will load the data using Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRpV_tdTa6qD"
   },
   "outputs": [],
   "source": [
    "Signals=[] # This will contain the signals generated from audio files\n",
    "for i in tnrange(data.shape[0]):\n",
    "    path = '{}.flac'.format(data['recording_id'][i])\n",
    "    audio_signal,sample_rate=librosa.load(path,sr=None) #Audio files are already encoded with sample rate of \n",
    "                                                   #  48000, so we don't need to resample them.\n",
    "    signal=audio_signal[int(data['t_min'][i]*48000):int(data['t_max'][i]*48000)+1] # Slicing audio files with exact time as given in the csv file \n",
    "    if(data['t_min'][i]-0.2)<0: # Adding 0.2 second to the left and right of the time given and then slicing the file\n",
    "        t1=0\n",
    "    else:\n",
    "        t1=int((data['t_min'][i]-0.2)*48000)\n",
    "\n",
    "    if(data['t_max'][i]+0.2)>60:\n",
    "        t2=60*48000\n",
    "    else:\n",
    "        t2=int((data['t_max'][i]+0.2)*48000)\n",
    "    extended_signal=audio_signal[t1:t2]\n",
    "    # Here we are slicing the audio files according to the time at which that species was heard.\n",
    "    Signals.append(signal)\n",
    "    Signals.append(extended_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AAgy0o76E0J"
   },
   "outputs": [],
   "source": [
    "# Now we have two files for each recording, so creating y_label accordingly\n",
    "y_label=[]\n",
    "for i in data['species_id']:\n",
    "    y_label.append(i)\n",
    "    y_label.append(i)\n",
    "print(len(y_label))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7LYehmy6UKw"
   },
   "outputs": [],
   "source": [
    "# Now, we will do 80:20 stratified split for train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train,y_test = train_test_split(pd.DataFrame(y_label),stratify=y_label,test_size=0.30,random_state=45)\n",
    "\n",
    "# Splitting spectrogram data into train and test\n",
    "train=[Signals[i] for i in y_train.index]\n",
    "test=[Signals[i] for i in y_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jSq7b4J6UHQ"
   },
   "outputs": [],
   "source": [
    "# Function for augmentation(We will apply augmentation only on train data) \n",
    "# We will apply two types of augmentation- Time stretch, pitch shift\n",
    "def augmentation(samples):\n",
    "    augmented_data = []\n",
    "    n=np.random.uniform()\n",
    "    if(n<0.25):\n",
    "        data=librosa.effects.time_stretch(samples, rate=0.7)\n",
    "    elif(n<0.5):\n",
    "        data=librosa.effects.time_stretch(samples, rate=1.3)\n",
    "    elif(n<0.75):\n",
    "        data=librosa.effects.pitch_shift(samples, sr=48000, n_steps=-1)\n",
    "    else:\n",
    "        data=librosa.effects.pitch_shift(samples, sr=48000, n_steps=1)        \n",
    "    augmented_data.append(data)\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBQzMQZG6UEM"
   },
   "outputs": [],
   "source": [
    "aug_train=[]\n",
    "for i in tnrange(len(train)):\n",
    "    aug_train.append(train[i]) # Appending original data point\n",
    "    aug_train.extend(augmentation(train[i])) # Appending augmented data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIKewzAy6UCV"
   },
   "outputs": [],
   "source": [
    "# Creating a new y_label according to our augmented data\n",
    "new_y_train=[]\n",
    "for i in y_train[0].values:\n",
    "    new_y_train.extend(i for k in range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rt4q_54pZmjt",
    "outputId": "bf75dcd8-b590-44f8-de58-4ffb1ab2f632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570776\n"
     ]
    }
   ],
   "source": [
    "max_length=max([len(i) for i in aug_train])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asZlTd5TZnei"
   },
   "outputs": [],
   "source": [
    "# Now we will write a function to convert raw data to spectrogram\n",
    "def spectrogram(raw_signal):\n",
    "    mel = librosa.feature.melspectrogram(y=raw_signal, sr=48000, n_mels=64)\n",
    "    log_spec = librosa.power_to_db(S=mel, ref=np.max)\n",
    "    return log_spec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qORDQV8JY56E"
   },
   "outputs": [],
   "source": [
    "# Now, we will write a function for padding our data to maximum length of audio sequence.\n",
    "def pad(data):\n",
    "    final_data=[]\n",
    "    for k in tqdm(data):\n",
    "        if(len(k)<max_length): # Checking if data has smaller length than maximum \n",
    "            k=list(k)\n",
    "            k.extend(0 for i in range(max_length-len(k)))    # Appending 0s to the data\n",
    "            final_data.append(spectrogram(np.array(k)))\n",
    "        else:\n",
    "            final_data.append(spectrogram(k)) \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brvEEQu-e16f"
   },
   "outputs": [],
   "source": [
    "final_data=pad(aug_train) # Padding the data\n",
    "test_data=pad(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NbF-wQzZnZl"
   },
   "outputs": [],
   "source": [
    "# Normalising train and test spectrogram data\n",
    "mean=np.mean(train_data,(0,1,2))\n",
    "std=np.std(train_data,(0,1,2))\n",
    "train_spec=(train_data-mean)/std\n",
    "test_spec=(test_data-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wivl1mDWf7t4"
   },
   "source": [
    "So we are done with all the preprocessing, now we should move ahead with modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saTZe-WLgGh_"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycHUmbvh13XW"
   },
   "source": [
    "Now, we will define the archtecture of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3usql6ju_T9U"
   },
   "source": [
    "### Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seslGurTZnV5"
   },
   "outputs": [],
   "source": [
    "input_layer=Input(shape=(train_spec.shape[1],train_spec.shape[2]))\n",
    "x=Conv1D(filters=24,kernel_size=5)(input_layer)\n",
    "x=MaxPooling1D(strides=2)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Conv1D(filters=48,kernel_size=5)(x)\n",
    "x=MaxPooling1D(strides=2)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Conv1D(filters=48,kernel_size=5)(x)\n",
    "x=GlobalAveragePooling1D()(x)\n",
    "x=Dense(64,activation='relu')(x) # Dense layer\n",
    "x=Dropout(0.3)(x) # Dropout layer\n",
    "output=Dense(24,activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[input_layer],outputs=output)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "optimizer=tf.keras.optimizers.Adam()  # Optimizer\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIu-aW8wEtoF",
    "outputId": "f1e9499d-4811-4215-f28b-963b913f1905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 1115)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 60, 24)            133824    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 30, 24)            0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 26, 48)            5808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 13, 48)            0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 9, 48)             11568     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                3136      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                1560      \n",
      "=================================================================\n",
      "Total params: 155,896\n",
      "Trainable params: 155,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VutvLMsbEtqf",
    "outputId": "02682874-6db7-4708-adff-892b20e6f8f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "107/107 [==============================] - 9s 16ms/step - loss: 2.9364 - accuracy: 0.1164 - val_loss: 2.1165 - val_accuracy: 0.2945\n",
      "Epoch 2/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 2.3895 - accuracy: 0.2079 - val_loss: 1.8770 - val_accuracy: 0.3247\n",
      "Epoch 3/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 2.2353 - accuracy: 0.2583 - val_loss: 1.6764 - val_accuracy: 0.3863\n",
      "Epoch 4/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 2.0831 - accuracy: 0.2718 - val_loss: 1.4942 - val_accuracy: 0.4548\n",
      "Epoch 5/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 1.9478 - accuracy: 0.3380 - val_loss: 1.4925 - val_accuracy: 0.4493\n",
      "Epoch 6/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 1.8320 - accuracy: 0.3910 - val_loss: 1.3714 - val_accuracy: 0.5521\n",
      "Epoch 7/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.7261 - accuracy: 0.4289 - val_loss: 1.2235 - val_accuracy: 0.5836\n",
      "Epoch 8/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.6152 - accuracy: 0.4748 - val_loss: 1.0321 - val_accuracy: 0.6753\n",
      "Epoch 9/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 1.5037 - accuracy: 0.4934 - val_loss: 0.9878 - val_accuracy: 0.6753\n",
      "Epoch 10/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 1.4337 - accuracy: 0.5193 - val_loss: 0.8881 - val_accuracy: 0.6877\n",
      "Epoch 11/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 1.3387 - accuracy: 0.5610 - val_loss: 0.8030 - val_accuracy: 0.7342\n",
      "Epoch 12/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.2534 - accuracy: 0.5874 - val_loss: 1.0857 - val_accuracy: 0.6027\n",
      "Epoch 13/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 1.3064 - accuracy: 0.5568 - val_loss: 0.7169 - val_accuracy: 0.7712\n",
      "Epoch 14/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.2121 - accuracy: 0.5853 - val_loss: 0.7215 - val_accuracy: 0.7425\n",
      "Epoch 15/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.1395 - accuracy: 0.6152 - val_loss: 0.6624 - val_accuracy: 0.7836\n",
      "Epoch 16/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.0903 - accuracy: 0.6409 - val_loss: 0.6713 - val_accuracy: 0.7890\n",
      "Epoch 17/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.0828 - accuracy: 0.6433 - val_loss: 0.7283 - val_accuracy: 0.7589\n",
      "Epoch 18/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.0166 - accuracy: 0.6690 - val_loss: 0.6564 - val_accuracy: 0.8041\n",
      "Epoch 19/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 1.0434 - accuracy: 0.6722 - val_loss: 0.6205 - val_accuracy: 0.8014\n",
      "Epoch 20/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.9961 - accuracy: 0.6811 - val_loss: 0.6234 - val_accuracy: 0.7836\n",
      "Epoch 21/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 1.0219 - accuracy: 0.6711 - val_loss: 0.5615 - val_accuracy: 0.8137\n",
      "Epoch 22/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.8658 - accuracy: 0.7193 - val_loss: 0.5154 - val_accuracy: 0.8548\n",
      "Epoch 23/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.8961 - accuracy: 0.7114 - val_loss: 0.4791 - val_accuracy: 0.8397\n",
      "Epoch 24/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.8399 - accuracy: 0.7270 - val_loss: 0.4785 - val_accuracy: 0.8671\n",
      "Epoch 25/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.8206 - accuracy: 0.7350 - val_loss: 0.5229 - val_accuracy: 0.8438\n",
      "Epoch 26/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.8380 - accuracy: 0.7401 - val_loss: 0.4507 - val_accuracy: 0.8699\n",
      "Epoch 27/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.8410 - accuracy: 0.7351 - val_loss: 0.6290 - val_accuracy: 0.7808\n",
      "Epoch 28/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.8373 - accuracy: 0.7326 - val_loss: 0.4124 - val_accuracy: 0.8726\n",
      "Epoch 29/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.7226 - accuracy: 0.7714 - val_loss: 0.5112 - val_accuracy: 0.8507\n",
      "Epoch 30/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.7061 - accuracy: 0.7670 - val_loss: 0.4461 - val_accuracy: 0.8603\n",
      "Epoch 31/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.7001 - accuracy: 0.7800 - val_loss: 0.4854 - val_accuracy: 0.8493\n",
      "Epoch 32/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.7719 - accuracy: 0.7524 - val_loss: 0.4330 - val_accuracy: 0.8685\n",
      "Epoch 33/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6737 - accuracy: 0.7860 - val_loss: 0.4556 - val_accuracy: 0.8630\n",
      "Epoch 34/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.6909 - accuracy: 0.7754 - val_loss: 0.3604 - val_accuracy: 0.8918\n",
      "Epoch 35/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6868 - accuracy: 0.7832 - val_loss: 0.3903 - val_accuracy: 0.8781\n",
      "Epoch 36/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.6290 - accuracy: 0.7944 - val_loss: 0.4289 - val_accuracy: 0.8836\n",
      "Epoch 37/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.7205 - accuracy: 0.7631 - val_loss: 0.4879 - val_accuracy: 0.8685\n",
      "Epoch 38/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.6251 - accuracy: 0.7946 - val_loss: 0.4180 - val_accuracy: 0.8658\n",
      "Epoch 39/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6367 - accuracy: 0.7954 - val_loss: 0.3744 - val_accuracy: 0.8877\n",
      "Epoch 40/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6271 - accuracy: 0.7996 - val_loss: 0.3551 - val_accuracy: 0.9014\n",
      "Epoch 41/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6348 - accuracy: 0.7983 - val_loss: 0.3878 - val_accuracy: 0.8973\n",
      "Epoch 42/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.5703 - accuracy: 0.8116 - val_loss: 0.3460 - val_accuracy: 0.8863\n",
      "Epoch 43/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.5357 - accuracy: 0.8234 - val_loss: 0.3791 - val_accuracy: 0.8808\n",
      "Epoch 44/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.5328 - accuracy: 0.8187 - val_loss: 0.3067 - val_accuracy: 0.9178\n",
      "Epoch 45/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.7293 - accuracy: 0.7948 - val_loss: 0.3930 - val_accuracy: 0.8781\n",
      "Epoch 46/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6375 - accuracy: 0.7974 - val_loss: 0.3034 - val_accuracy: 0.9164\n",
      "Epoch 47/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.5634 - accuracy: 0.8173 - val_loss: 0.3401 - val_accuracy: 0.8945\n",
      "Epoch 48/400\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.5412 - accuracy: 0.8324 - val_loss: 0.3030 - val_accuracy: 0.9123\n",
      "Epoch 49/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.5671 - accuracy: 0.8111 - val_loss: 0.3596 - val_accuracy: 0.8863\n",
      "Epoch 50/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4890 - accuracy: 0.8380 - val_loss: 0.6045 - val_accuracy: 0.8233\n",
      "Epoch 51/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6230 - accuracy: 0.7984 - val_loss: 0.3639 - val_accuracy: 0.8904\n",
      "Epoch 52/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4898 - accuracy: 0.8333 - val_loss: 0.3255 - val_accuracy: 0.9068\n",
      "Epoch 53/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4941 - accuracy: 0.8381 - val_loss: 0.2947 - val_accuracy: 0.9137\n",
      "Epoch 54/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4924 - accuracy: 0.8391 - val_loss: 0.2949 - val_accuracy: 0.9219\n",
      "Epoch 55/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4757 - accuracy: 0.8456 - val_loss: 0.2694 - val_accuracy: 0.9301\n",
      "Epoch 56/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4535 - accuracy: 0.8513 - val_loss: 0.2878 - val_accuracy: 0.9178\n",
      "Epoch 57/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4946 - accuracy: 0.8343 - val_loss: 0.6086 - val_accuracy: 0.8123\n",
      "Epoch 58/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.5227 - accuracy: 0.8273 - val_loss: 0.2506 - val_accuracy: 0.9342\n",
      "Epoch 59/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4688 - accuracy: 0.8382 - val_loss: 0.2334 - val_accuracy: 0.9384\n",
      "Epoch 60/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3834 - accuracy: 0.8733 - val_loss: 0.3346 - val_accuracy: 0.8822\n",
      "Epoch 61/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.5417 - accuracy: 0.8188 - val_loss: 0.3328 - val_accuracy: 0.9041\n",
      "Epoch 62/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4491 - accuracy: 0.8498 - val_loss: 0.3526 - val_accuracy: 0.8890\n",
      "Epoch 63/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4644 - accuracy: 0.8439 - val_loss: 0.2877 - val_accuracy: 0.9192\n",
      "Epoch 64/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4653 - accuracy: 0.8480 - val_loss: 0.2521 - val_accuracy: 0.9247\n",
      "Epoch 65/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4152 - accuracy: 0.8547 - val_loss: 0.3159 - val_accuracy: 0.9041\n",
      "Epoch 66/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4214 - accuracy: 0.8606 - val_loss: 0.3664 - val_accuracy: 0.8863\n",
      "Epoch 67/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4200 - accuracy: 0.8667 - val_loss: 0.2620 - val_accuracy: 0.9233\n",
      "Epoch 68/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3858 - accuracy: 0.8798 - val_loss: 0.2578 - val_accuracy: 0.9301\n",
      "Epoch 69/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3349 - accuracy: 0.8877 - val_loss: 0.2659 - val_accuracy: 0.9356\n",
      "Epoch 70/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3891 - accuracy: 0.8740 - val_loss: 0.2696 - val_accuracy: 0.9219\n",
      "Epoch 71/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3993 - accuracy: 0.8747 - val_loss: 0.3756 - val_accuracy: 0.8904\n",
      "Epoch 72/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.4855 - accuracy: 0.8373 - val_loss: 0.3597 - val_accuracy: 0.8973\n",
      "Epoch 73/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.5660 - accuracy: 0.8273 - val_loss: 0.3415 - val_accuracy: 0.9123\n",
      "Epoch 74/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4235 - accuracy: 0.8564 - val_loss: 0.3413 - val_accuracy: 0.9151\n",
      "Epoch 75/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4207 - accuracy: 0.8632 - val_loss: 0.2911 - val_accuracy: 0.9192\n",
      "Epoch 76/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3650 - accuracy: 0.8727 - val_loss: 0.2694 - val_accuracy: 0.9301\n",
      "Epoch 77/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4472 - accuracy: 0.8503 - val_loss: 0.2715 - val_accuracy: 0.9329\n",
      "Epoch 78/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3677 - accuracy: 0.8807 - val_loss: 0.3085 - val_accuracy: 0.9219\n",
      "Epoch 79/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4447 - accuracy: 0.8638 - val_loss: 0.3242 - val_accuracy: 0.9137\n",
      "Epoch 80/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3842 - accuracy: 0.8758 - val_loss: 0.3427 - val_accuracy: 0.9123\n",
      "Epoch 81/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.5032 - accuracy: 0.8337 - val_loss: 0.3038 - val_accuracy: 0.9096\n",
      "Epoch 82/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3559 - accuracy: 0.8728 - val_loss: 0.2919 - val_accuracy: 0.9301\n",
      "Epoch 83/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3746 - accuracy: 0.8756 - val_loss: 0.2660 - val_accuracy: 0.9329\n",
      "Epoch 84/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3260 - accuracy: 0.8937 - val_loss: 0.2435 - val_accuracy: 0.9384\n",
      "Epoch 85/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2936 - accuracy: 0.9010 - val_loss: 0.2497 - val_accuracy: 0.9425\n",
      "Epoch 86/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3747 - accuracy: 0.8675 - val_loss: 0.2542 - val_accuracy: 0.9192\n",
      "Epoch 87/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3785 - accuracy: 0.8768 - val_loss: 0.3566 - val_accuracy: 0.9068\n",
      "Epoch 88/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3417 - accuracy: 0.8851 - val_loss: 0.2419 - val_accuracy: 0.9466\n",
      "Epoch 89/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2863 - accuracy: 0.9071 - val_loss: 0.3238 - val_accuracy: 0.9110\n",
      "Epoch 90/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3817 - accuracy: 0.8870 - val_loss: 0.2428 - val_accuracy: 0.9411\n",
      "Epoch 91/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3498 - accuracy: 0.8842 - val_loss: 0.2604 - val_accuracy: 0.9260\n",
      "Epoch 92/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3448 - accuracy: 0.8871 - val_loss: 0.2356 - val_accuracy: 0.9384\n",
      "Epoch 93/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.4025 - accuracy: 0.8628 - val_loss: 0.3452 - val_accuracy: 0.9027\n",
      "Epoch 94/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3178 - accuracy: 0.8960 - val_loss: 0.2469 - val_accuracy: 0.9315\n",
      "Epoch 95/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3121 - accuracy: 0.8867 - val_loss: 0.3227 - val_accuracy: 0.9068\n",
      "Epoch 96/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3059 - accuracy: 0.9056 - val_loss: 0.2562 - val_accuracy: 0.9260\n",
      "Epoch 97/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.5236 - accuracy: 0.8494 - val_loss: 0.3006 - val_accuracy: 0.9178\n",
      "Epoch 98/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3656 - accuracy: 0.8860 - val_loss: 0.2374 - val_accuracy: 0.9397\n",
      "Epoch 99/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2782 - accuracy: 0.9117 - val_loss: 0.3037 - val_accuracy: 0.9247\n",
      "Epoch 100/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3242 - accuracy: 0.8876 - val_loss: 0.2909 - val_accuracy: 0.9274\n",
      "Epoch 101/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2886 - accuracy: 0.9012 - val_loss: 0.2573 - val_accuracy: 0.9301\n",
      "Epoch 102/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2446 - accuracy: 0.9195 - val_loss: 0.2549 - val_accuracy: 0.9397\n",
      "Epoch 103/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2931 - accuracy: 0.9121 - val_loss: 0.2458 - val_accuracy: 0.9384\n",
      "Epoch 104/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2967 - accuracy: 0.8955 - val_loss: 0.3825 - val_accuracy: 0.8932\n",
      "Epoch 105/400\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.4318 - accuracy: 0.8598 - val_loss: 0.3581 - val_accuracy: 0.8918\n",
      "Epoch 106/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3291 - accuracy: 0.8796 - val_loss: 0.3015 - val_accuracy: 0.9164\n",
      "Epoch 107/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3764 - accuracy: 0.8744 - val_loss: 0.2987 - val_accuracy: 0.9260\n",
      "Epoch 108/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3664 - accuracy: 0.8766 - val_loss: 0.2388 - val_accuracy: 0.9425\n",
      "Epoch 109/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3169 - accuracy: 0.8958 - val_loss: 0.3129 - val_accuracy: 0.9260\n",
      "Epoch 110/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2755 - accuracy: 0.9123 - val_loss: 0.2236 - val_accuracy: 0.9534\n",
      "Epoch 111/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2569 - accuracy: 0.9119 - val_loss: 0.2970 - val_accuracy: 0.9247\n",
      "Epoch 112/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2754 - accuracy: 0.8980 - val_loss: 0.3037 - val_accuracy: 0.9260\n",
      "Epoch 113/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3241 - accuracy: 0.9003 - val_loss: 0.3010 - val_accuracy: 0.9329\n",
      "Epoch 114/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2962 - accuracy: 0.9059 - val_loss: 0.2409 - val_accuracy: 0.9479\n",
      "Epoch 115/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3377 - accuracy: 0.8929 - val_loss: 0.2180 - val_accuracy: 0.9384\n",
      "Epoch 116/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3159 - accuracy: 0.9015 - val_loss: 0.2858 - val_accuracy: 0.9274\n",
      "Epoch 117/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2335 - accuracy: 0.9218 - val_loss: 0.3086 - val_accuracy: 0.9219\n",
      "Epoch 118/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2539 - accuracy: 0.9194 - val_loss: 0.3155 - val_accuracy: 0.9260\n",
      "Epoch 119/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2492 - accuracy: 0.9192 - val_loss: 0.2699 - val_accuracy: 0.9205\n",
      "Epoch 120/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3302 - accuracy: 0.8928 - val_loss: 0.2141 - val_accuracy: 0.9452\n",
      "Epoch 121/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2505 - accuracy: 0.9137 - val_loss: 0.2825 - val_accuracy: 0.9370\n",
      "Epoch 122/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2622 - accuracy: 0.9156 - val_loss: 0.2406 - val_accuracy: 0.9411\n",
      "Epoch 123/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2478 - accuracy: 0.9122 - val_loss: 0.2817 - val_accuracy: 0.9384\n",
      "Epoch 124/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3364 - accuracy: 0.8972 - val_loss: 0.2619 - val_accuracy: 0.9370\n",
      "Epoch 125/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2615 - accuracy: 0.9163 - val_loss: 0.2769 - val_accuracy: 0.9342\n",
      "Epoch 126/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2505 - accuracy: 0.9135 - val_loss: 0.5574 - val_accuracy: 0.8740\n",
      "Epoch 127/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3339 - accuracy: 0.9053 - val_loss: 0.3542 - val_accuracy: 0.9027\n",
      "Epoch 128/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3128 - accuracy: 0.8933 - val_loss: 0.3158 - val_accuracy: 0.9274\n",
      "Epoch 129/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2677 - accuracy: 0.9133 - val_loss: 0.3138 - val_accuracy: 0.9329\n",
      "Epoch 130/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2527 - accuracy: 0.9210 - val_loss: 0.2842 - val_accuracy: 0.9329\n",
      "Epoch 131/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2723 - accuracy: 0.8978 - val_loss: 0.2877 - val_accuracy: 0.9384\n",
      "Epoch 132/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3420 - accuracy: 0.8862 - val_loss: 0.2804 - val_accuracy: 0.9329\n",
      "Epoch 133/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2910 - accuracy: 0.9018 - val_loss: 0.2582 - val_accuracy: 0.9466\n",
      "Epoch 134/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2809 - accuracy: 0.9071 - val_loss: 0.2768 - val_accuracy: 0.9315\n",
      "Epoch 135/400\n",
      "107/107 [==============================] - 1s 11ms/step - loss: 0.2502 - accuracy: 0.9117 - val_loss: 0.2518 - val_accuracy: 0.9397\n",
      "Epoch 136/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2342 - accuracy: 0.9278 - val_loss: 0.2491 - val_accuracy: 0.9384\n",
      "Epoch 137/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2776 - accuracy: 0.9108 - val_loss: 0.3391 - val_accuracy: 0.9110\n",
      "Epoch 138/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2315 - accuracy: 0.9246 - val_loss: 0.3575 - val_accuracy: 0.9178\n",
      "Epoch 139/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3062 - accuracy: 0.9063 - val_loss: 0.3388 - val_accuracy: 0.9274\n",
      "Epoch 140/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3783 - accuracy: 0.8948 - val_loss: 0.2751 - val_accuracy: 0.9301\n",
      "Epoch 141/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2407 - accuracy: 0.9233 - val_loss: 0.2468 - val_accuracy: 0.9452\n",
      "Epoch 142/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2727 - accuracy: 0.9117 - val_loss: 0.2917 - val_accuracy: 0.9288\n",
      "Epoch 143/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2360 - accuracy: 0.9209 - val_loss: 0.2643 - val_accuracy: 0.9534\n",
      "Epoch 144/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2711 - accuracy: 0.9084 - val_loss: 0.2966 - val_accuracy: 0.9452\n",
      "Epoch 145/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2362 - accuracy: 0.9159 - val_loss: 0.2605 - val_accuracy: 0.9438\n",
      "Epoch 146/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2236 - accuracy: 0.9264 - val_loss: 0.3178 - val_accuracy: 0.9397\n",
      "Epoch 147/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2479 - accuracy: 0.9165 - val_loss: 0.2974 - val_accuracy: 0.9288\n",
      "Epoch 148/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3246 - accuracy: 0.8946 - val_loss: 0.2788 - val_accuracy: 0.9493\n",
      "Epoch 149/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.1852 - accuracy: 0.9345 - val_loss: 0.2974 - val_accuracy: 0.9192\n",
      "Epoch 150/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2941 - accuracy: 0.9023 - val_loss: 0.4884 - val_accuracy: 0.8932\n",
      "Epoch 151/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.3824 - accuracy: 0.8804 - val_loss: 0.3608 - val_accuracy: 0.9082\n",
      "Epoch 152/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2076 - accuracy: 0.9242 - val_loss: 0.2667 - val_accuracy: 0.9315\n",
      "Epoch 153/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2442 - accuracy: 0.9108 - val_loss: 0.3301 - val_accuracy: 0.9000\n",
      "Epoch 154/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2070 - accuracy: 0.9302 - val_loss: 0.3165 - val_accuracy: 0.9123\n",
      "Epoch 155/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.3419 - accuracy: 0.8904 - val_loss: 0.2934 - val_accuracy: 0.9479\n",
      "Epoch 156/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2542 - accuracy: 0.9170 - val_loss: 0.2479 - val_accuracy: 0.9425\n",
      "Epoch 157/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2040 - accuracy: 0.9263 - val_loss: 0.2316 - val_accuracy: 0.9534\n",
      "Epoch 158/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2773 - accuracy: 0.9120 - val_loss: 0.2557 - val_accuracy: 0.9384\n",
      "Epoch 159/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.2400 - accuracy: 0.9205 - val_loss: 0.3245 - val_accuracy: 0.9178\n",
      "Epoch 160/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2771 - accuracy: 0.9004 - val_loss: 0.2662 - val_accuracy: 0.9425\n",
      "Epoch 161/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.2048 - accuracy: 0.9311 - val_loss: 0.2645 - val_accuracy: 0.9342\n",
      "Epoch 162/400\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.1872 - accuracy: 0.9304 - val_loss: 0.2102 - val_accuracy: 0.9562\n",
      "Epoch 163/400\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.1553 - accuracy: 0.9503 - val_loss: 0.3196 - val_accuracy: 0.9151\n",
      "Epoch 164/400\n",
      " 13/107 [==>...........................] - ETA: 0s - loss: 0.2283 - accuracy: 0.9233"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8d32b6d978e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "model.fit(train_spec,y_train,epochs=400,validation_data=(test_spec,y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI1vyE712-OW"
   },
   "source": [
    "We have trained our model and checked it's performance using cross entropy and accuracy, both of which indicated good performance.        \n",
    "Now, we will try a different model architecture to see if it can increase performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB197ga0BhpC"
   },
   "source": [
    "### LSTM Model-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iv2sXtXGBg7z"
   },
   "outputs": [],
   "source": [
    "## as discussed above, please write the LSTM\n",
    "!rm -rf ./logs/\n",
    "input_layer=Input(shape=(train_spec.shape[1],train_spec.shape[2]))   # Input layer\n",
    "lstm_layer=LSTM(units=50,return_sequences=True)(input_layer)  # LSTM layer with 25 units\n",
    "average=GlobalAveragePooling1D()(lstm_layer) #Global average pool\n",
    "layer_1=Dense(32,activation='relu')(average) # Dense layer\n",
    "drop=Dropout(0.3)(layer_1) # Dropout layer\n",
    "output=Dense(24,activation='softmax')(drop) # output layer\n",
    "\n",
    "model_spec = Model(inputs=[input_layer],outputs=output)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "optimizer=tf.keras.optimizers.Adam()  # Optimizer\n",
    "model_spec.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',metrics='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9uSMg5ACT15",
    "outputId": "a7b888eb-8032-4e15-f9c8-15d807c6fab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 1115)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 60, 24)            133824    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 30, 24)            0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 26, 48)            5808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 13, 48)            0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 9, 48)             11568     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                3136      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                1560      \n",
      "=================================================================\n",
      "Total params: 155,896\n",
      "Trainable params: 155,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aPnOmC3oXDhx",
    "outputId": "9429d46c-ab70-4abd-c922-9dc9f758ee12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "107/107 [==============================] - 4s 21ms/step - loss: 2.8812 - accuracy: 0.1661 - val_loss: 2.2963 - val_accuracy: 0.2863\n",
      "Epoch 2/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 2.4065 - accuracy: 0.2536 - val_loss: 1.9307 - val_accuracy: 0.3849\n",
      "Epoch 3/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 2.1896 - accuracy: 0.2885 - val_loss: 1.6625 - val_accuracy: 0.4438\n",
      "Epoch 4/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 2.0391 - accuracy: 0.3299 - val_loss: 1.5908 - val_accuracy: 0.4767\n",
      "Epoch 5/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 1.9396 - accuracy: 0.3673 - val_loss: 1.4370 - val_accuracy: 0.5014\n",
      "Epoch 6/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 1.8456 - accuracy: 0.3938 - val_loss: 1.3717 - val_accuracy: 0.5932\n",
      "Epoch 7/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.7972 - accuracy: 0.4002 - val_loss: 1.2973 - val_accuracy: 0.6041\n",
      "Epoch 8/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.7095 - accuracy: 0.4512 - val_loss: 1.2085 - val_accuracy: 0.6479\n",
      "Epoch 9/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.6374 - accuracy: 0.4627 - val_loss: 1.2097 - val_accuracy: 0.5945\n",
      "Epoch 10/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.5570 - accuracy: 0.4916 - val_loss: 1.0370 - val_accuracy: 0.7164\n",
      "Epoch 11/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.5294 - accuracy: 0.5187 - val_loss: 1.0327 - val_accuracy: 0.7534\n",
      "Epoch 12/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.4807 - accuracy: 0.5291 - val_loss: 0.9412 - val_accuracy: 0.7534\n",
      "Epoch 13/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.4168 - accuracy: 0.5594 - val_loss: 0.9184 - val_accuracy: 0.7616\n",
      "Epoch 14/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 1.4150 - accuracy: 0.5518 - val_loss: 0.8341 - val_accuracy: 0.7959\n",
      "Epoch 15/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.3141 - accuracy: 0.5860 - val_loss: 0.8266 - val_accuracy: 0.7452\n",
      "Epoch 16/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.3037 - accuracy: 0.5960 - val_loss: 0.7864 - val_accuracy: 0.7836\n",
      "Epoch 17/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.2842 - accuracy: 0.5897 - val_loss: 0.7652 - val_accuracy: 0.7712\n",
      "Epoch 18/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 1.2274 - accuracy: 0.6153 - val_loss: 0.7249 - val_accuracy: 0.7945\n",
      "Epoch 19/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 1.1791 - accuracy: 0.6362 - val_loss: 0.6894 - val_accuracy: 0.8178\n",
      "Epoch 20/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 1.1881 - accuracy: 0.6200 - val_loss: 0.6339 - val_accuracy: 0.8301\n",
      "Epoch 21/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.1171 - accuracy: 0.6542 - val_loss: 0.6451 - val_accuracy: 0.8068\n",
      "Epoch 22/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 1.1523 - accuracy: 0.6275 - val_loss: 0.6035 - val_accuracy: 0.8315\n",
      "Epoch 23/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 1.1032 - accuracy: 0.6520 - val_loss: 0.6012 - val_accuracy: 0.8178\n",
      "Epoch 24/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.0796 - accuracy: 0.6746 - val_loss: 0.6269 - val_accuracy: 0.8247\n",
      "Epoch 25/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.0277 - accuracy: 0.6883 - val_loss: 0.6240 - val_accuracy: 0.8151\n",
      "Epoch 26/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.0522 - accuracy: 0.6599 - val_loss: 0.5545 - val_accuracy: 0.8479\n",
      "Epoch 27/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 1.0116 - accuracy: 0.6746 - val_loss: 0.5706 - val_accuracy: 0.8425\n",
      "Epoch 28/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.9668 - accuracy: 0.6924 - val_loss: 0.5323 - val_accuracy: 0.8781\n",
      "Epoch 29/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.9890 - accuracy: 0.6879 - val_loss: 0.4756 - val_accuracy: 0.8890\n",
      "Epoch 30/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.9554 - accuracy: 0.6922 - val_loss: 0.4834 - val_accuracy: 0.8603\n",
      "Epoch 31/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.9586 - accuracy: 0.6948 - val_loss: 0.4544 - val_accuracy: 0.8877\n",
      "Epoch 32/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.9086 - accuracy: 0.7153 - val_loss: 0.4947 - val_accuracy: 0.8493\n",
      "Epoch 33/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.8584 - accuracy: 0.7282 - val_loss: 0.4251 - val_accuracy: 0.8795\n",
      "Epoch 34/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.8882 - accuracy: 0.7302 - val_loss: 0.4263 - val_accuracy: 0.8808\n",
      "Epoch 35/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.8554 - accuracy: 0.7199 - val_loss: 0.4511 - val_accuracy: 0.8644\n",
      "Epoch 36/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.8988 - accuracy: 0.7130 - val_loss: 0.4253 - val_accuracy: 0.8795\n",
      "Epoch 37/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.8182 - accuracy: 0.7501 - val_loss: 0.4345 - val_accuracy: 0.8616\n",
      "Epoch 38/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.8314 - accuracy: 0.7322 - val_loss: 0.4134 - val_accuracy: 0.8753\n",
      "Epoch 39/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.7791 - accuracy: 0.7526 - val_loss: 0.3874 - val_accuracy: 0.8808\n",
      "Epoch 40/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.7747 - accuracy: 0.7361 - val_loss: 0.3965 - val_accuracy: 0.8740\n",
      "Epoch 41/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.7849 - accuracy: 0.7491 - val_loss: 0.3561 - val_accuracy: 0.8904\n",
      "Epoch 42/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.7391 - accuracy: 0.7606 - val_loss: 0.3681 - val_accuracy: 0.8904\n",
      "Epoch 43/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.7716 - accuracy: 0.7492 - val_loss: 0.3617 - val_accuracy: 0.8973\n",
      "Epoch 44/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6874 - accuracy: 0.7841 - val_loss: 0.3592 - val_accuracy: 0.8973\n",
      "Epoch 45/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.7587 - accuracy: 0.7560 - val_loss: 0.3540 - val_accuracy: 0.8932\n",
      "Epoch 46/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.7441 - accuracy: 0.7569 - val_loss: 0.4352 - val_accuracy: 0.8712\n",
      "Epoch 47/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.7113 - accuracy: 0.7650 - val_loss: 0.3390 - val_accuracy: 0.8849\n",
      "Epoch 48/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.6901 - accuracy: 0.7857 - val_loss: 0.3300 - val_accuracy: 0.8945\n",
      "Epoch 49/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.6394 - accuracy: 0.7878 - val_loss: 0.3065 - val_accuracy: 0.9110\n",
      "Epoch 50/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6679 - accuracy: 0.7975 - val_loss: 0.3307 - val_accuracy: 0.8890\n",
      "Epoch 51/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6375 - accuracy: 0.7967 - val_loss: 0.3131 - val_accuracy: 0.8959\n",
      "Epoch 52/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.6408 - accuracy: 0.7966 - val_loss: 0.3172 - val_accuracy: 0.9068\n",
      "Epoch 53/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6682 - accuracy: 0.7757 - val_loss: 0.2915 - val_accuracy: 0.9247\n",
      "Epoch 54/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6211 - accuracy: 0.8033 - val_loss: 0.2853 - val_accuracy: 0.9192\n",
      "Epoch 55/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6156 - accuracy: 0.7924 - val_loss: 0.2882 - val_accuracy: 0.9205\n",
      "Epoch 56/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6296 - accuracy: 0.7926 - val_loss: 0.2882 - val_accuracy: 0.9082\n",
      "Epoch 57/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6220 - accuracy: 0.7872 - val_loss: 0.3020 - val_accuracy: 0.8959\n",
      "Epoch 58/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6305 - accuracy: 0.7961 - val_loss: 0.2779 - val_accuracy: 0.8986\n",
      "Epoch 59/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.6073 - accuracy: 0.8087 - val_loss: 0.2856 - val_accuracy: 0.9055\n",
      "Epoch 60/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5851 - accuracy: 0.8005 - val_loss: 0.2921 - val_accuracy: 0.9096\n",
      "Epoch 61/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6047 - accuracy: 0.8102 - val_loss: 0.3102 - val_accuracy: 0.9096\n",
      "Epoch 62/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5712 - accuracy: 0.8153 - val_loss: 0.3230 - val_accuracy: 0.8986\n",
      "Epoch 63/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6222 - accuracy: 0.7935 - val_loss: 0.2784 - val_accuracy: 0.9096\n",
      "Epoch 64/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6003 - accuracy: 0.7967 - val_loss: 0.4806 - val_accuracy: 0.8219\n",
      "Epoch 65/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.6455 - accuracy: 0.7864 - val_loss: 0.3115 - val_accuracy: 0.9055\n",
      "Epoch 66/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5637 - accuracy: 0.8216 - val_loss: 0.2740 - val_accuracy: 0.9137\n",
      "Epoch 67/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5486 - accuracy: 0.8121 - val_loss: 0.2610 - val_accuracy: 0.9178\n",
      "Epoch 68/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5447 - accuracy: 0.8210 - val_loss: 0.2409 - val_accuracy: 0.9123\n",
      "Epoch 69/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5626 - accuracy: 0.8170 - val_loss: 0.2677 - val_accuracy: 0.9110\n",
      "Epoch 70/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5188 - accuracy: 0.8287 - val_loss: 0.2807 - val_accuracy: 0.9041\n",
      "Epoch 71/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5527 - accuracy: 0.8192 - val_loss: 0.2680 - val_accuracy: 0.9041\n",
      "Epoch 72/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5553 - accuracy: 0.8163 - val_loss: 0.2424 - val_accuracy: 0.9260\n",
      "Epoch 73/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5610 - accuracy: 0.8139 - val_loss: 0.2712 - val_accuracy: 0.9123\n",
      "Epoch 74/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5581 - accuracy: 0.8125 - val_loss: 0.2467 - val_accuracy: 0.9082\n",
      "Epoch 75/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5255 - accuracy: 0.8244 - val_loss: 0.2500 - val_accuracy: 0.9151\n",
      "Epoch 76/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5271 - accuracy: 0.8288 - val_loss: 0.2482 - val_accuracy: 0.9137\n",
      "Epoch 77/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5020 - accuracy: 0.8369 - val_loss: 0.2503 - val_accuracy: 0.9178\n",
      "Epoch 78/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5540 - accuracy: 0.8068 - val_loss: 0.2495 - val_accuracy: 0.9301\n",
      "Epoch 79/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5448 - accuracy: 0.8161 - val_loss: 0.2682 - val_accuracy: 0.9082\n",
      "Epoch 80/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.5371 - accuracy: 0.8200 - val_loss: 0.2366 - val_accuracy: 0.9247\n",
      "Epoch 81/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4841 - accuracy: 0.8399 - val_loss: 0.2369 - val_accuracy: 0.9274\n",
      "Epoch 82/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4842 - accuracy: 0.8252 - val_loss: 0.2485 - val_accuracy: 0.9219\n",
      "Epoch 83/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5484 - accuracy: 0.8160 - val_loss: 0.2311 - val_accuracy: 0.9137\n",
      "Epoch 84/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5001 - accuracy: 0.8288 - val_loss: 0.2412 - val_accuracy: 0.9247\n",
      "Epoch 85/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.5198 - accuracy: 0.8330 - val_loss: 0.2208 - val_accuracy: 0.9178\n",
      "Epoch 86/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4585 - accuracy: 0.8470 - val_loss: 0.2960 - val_accuracy: 0.8877\n",
      "Epoch 87/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5468 - accuracy: 0.8149 - val_loss: 0.2266 - val_accuracy: 0.9274\n",
      "Epoch 88/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5040 - accuracy: 0.8232 - val_loss: 0.2558 - val_accuracy: 0.9110\n",
      "Epoch 89/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4494 - accuracy: 0.8481 - val_loss: 0.2982 - val_accuracy: 0.8986\n",
      "Epoch 90/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4952 - accuracy: 0.8248 - val_loss: 0.2276 - val_accuracy: 0.9233\n",
      "Epoch 91/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.4885 - accuracy: 0.8351 - val_loss: 0.2472 - val_accuracy: 0.9068\n",
      "Epoch 92/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5045 - accuracy: 0.8300 - val_loss: 0.2197 - val_accuracy: 0.9384\n",
      "Epoch 93/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5063 - accuracy: 0.8351 - val_loss: 0.2571 - val_accuracy: 0.9082\n",
      "Epoch 94/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4304 - accuracy: 0.8479 - val_loss: 0.2458 - val_accuracy: 0.9055\n",
      "Epoch 95/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4650 - accuracy: 0.8408 - val_loss: 0.2021 - val_accuracy: 0.9274\n",
      "Epoch 96/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4615 - accuracy: 0.8425 - val_loss: 0.2246 - val_accuracy: 0.9219\n",
      "Epoch 97/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5029 - accuracy: 0.8245 - val_loss: 0.2246 - val_accuracy: 0.9205\n",
      "Epoch 98/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.5237 - accuracy: 0.8237 - val_loss: 0.2484 - val_accuracy: 0.8945\n",
      "Epoch 99/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4820 - accuracy: 0.8439 - val_loss: 0.2447 - val_accuracy: 0.9301\n",
      "Epoch 100/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.5006 - accuracy: 0.8123 - val_loss: 0.2290 - val_accuracy: 0.9178\n",
      "Epoch 101/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4450 - accuracy: 0.8424 - val_loss: 0.2424 - val_accuracy: 0.9205\n",
      "Epoch 102/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.4402 - accuracy: 0.8492 - val_loss: 0.2017 - val_accuracy: 0.9315\n",
      "Epoch 103/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.4344 - accuracy: 0.8492 - val_loss: 0.2509 - val_accuracy: 0.9192\n",
      "Epoch 104/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4513 - accuracy: 0.8539 - val_loss: 0.2111 - val_accuracy: 0.9178\n",
      "Epoch 105/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4314 - accuracy: 0.8454 - val_loss: 0.2285 - val_accuracy: 0.9151\n",
      "Epoch 106/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4290 - accuracy: 0.8536 - val_loss: 0.2358 - val_accuracy: 0.9123\n",
      "Epoch 107/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4431 - accuracy: 0.8373 - val_loss: 0.2202 - val_accuracy: 0.9178\n",
      "Epoch 108/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4520 - accuracy: 0.8351 - val_loss: 0.3309 - val_accuracy: 0.8863\n",
      "Epoch 109/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4525 - accuracy: 0.8438 - val_loss: 0.2518 - val_accuracy: 0.9137\n",
      "Epoch 110/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4182 - accuracy: 0.8495 - val_loss: 0.2596 - val_accuracy: 0.9151\n",
      "Epoch 111/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4357 - accuracy: 0.8514 - val_loss: 0.2230 - val_accuracy: 0.9329\n",
      "Epoch 112/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3924 - accuracy: 0.8653 - val_loss: 0.2314 - val_accuracy: 0.9315\n",
      "Epoch 113/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4594 - accuracy: 0.8359 - val_loss: 0.1979 - val_accuracy: 0.9301\n",
      "Epoch 114/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4290 - accuracy: 0.8483 - val_loss: 0.2072 - val_accuracy: 0.9219\n",
      "Epoch 115/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3861 - accuracy: 0.8678 - val_loss: 0.2125 - val_accuracy: 0.9329\n",
      "Epoch 116/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.4006 - accuracy: 0.8519 - val_loss: 0.2189 - val_accuracy: 0.9288\n",
      "Epoch 117/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3721 - accuracy: 0.8759 - val_loss: 0.2739 - val_accuracy: 0.9000\n",
      "Epoch 118/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4198 - accuracy: 0.8588 - val_loss: 0.2246 - val_accuracy: 0.9151\n",
      "Epoch 119/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4867 - accuracy: 0.8389 - val_loss: 0.2567 - val_accuracy: 0.9288\n",
      "Epoch 120/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4273 - accuracy: 0.8507 - val_loss: 0.2055 - val_accuracy: 0.9205\n",
      "Epoch 121/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.4190 - accuracy: 0.8601 - val_loss: 0.2108 - val_accuracy: 0.9315\n",
      "Epoch 122/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4174 - accuracy: 0.8550 - val_loss: 0.2090 - val_accuracy: 0.9137\n",
      "Epoch 123/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4080 - accuracy: 0.8547 - val_loss: 0.2082 - val_accuracy: 0.9356\n",
      "Epoch 124/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3903 - accuracy: 0.8597 - val_loss: 0.2167 - val_accuracy: 0.9288\n",
      "Epoch 125/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3952 - accuracy: 0.8559 - val_loss: 0.2146 - val_accuracy: 0.9137\n",
      "Epoch 126/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3999 - accuracy: 0.8584 - val_loss: 0.2007 - val_accuracy: 0.9493\n",
      "Epoch 127/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4079 - accuracy: 0.8554 - val_loss: 0.2181 - val_accuracy: 0.9123\n",
      "Epoch 128/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4100 - accuracy: 0.8567 - val_loss: 0.2330 - val_accuracy: 0.9178\n",
      "Epoch 129/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.4040 - accuracy: 0.8534 - val_loss: 0.1965 - val_accuracy: 0.9288\n",
      "Epoch 130/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3760 - accuracy: 0.8647 - val_loss: 0.2743 - val_accuracy: 0.8959\n",
      "Epoch 131/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4608 - accuracy: 0.8333 - val_loss: 0.2097 - val_accuracy: 0.9370\n",
      "Epoch 132/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.3878 - accuracy: 0.8614 - val_loss: 0.1941 - val_accuracy: 0.9397\n",
      "Epoch 133/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3902 - accuracy: 0.8709 - val_loss: 0.2148 - val_accuracy: 0.9137\n",
      "Epoch 134/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4016 - accuracy: 0.8604 - val_loss: 0.2101 - val_accuracy: 0.9315\n",
      "Epoch 135/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4550 - accuracy: 0.8467 - val_loss: 0.1781 - val_accuracy: 0.9342\n",
      "Epoch 136/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4090 - accuracy: 0.8577 - val_loss: 0.1984 - val_accuracy: 0.9247\n",
      "Epoch 137/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.3737 - accuracy: 0.8675 - val_loss: 0.2071 - val_accuracy: 0.9260\n",
      "Epoch 138/300\n",
      "107/107 [==============================] - 2s 14ms/step - loss: 0.3834 - accuracy: 0.8585 - val_loss: 0.2459 - val_accuracy: 0.9137\n",
      "Epoch 139/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3718 - accuracy: 0.8698 - val_loss: 0.2407 - val_accuracy: 0.9178\n",
      "Epoch 140/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4033 - accuracy: 0.8629 - val_loss: 0.2332 - val_accuracy: 0.9342\n",
      "Epoch 141/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3733 - accuracy: 0.8694 - val_loss: 0.1946 - val_accuracy: 0.9438\n",
      "Epoch 142/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3487 - accuracy: 0.8684 - val_loss: 0.2321 - val_accuracy: 0.9164\n",
      "Epoch 143/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3607 - accuracy: 0.8730 - val_loss: 0.2272 - val_accuracy: 0.9192\n",
      "Epoch 144/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3867 - accuracy: 0.8513 - val_loss: 0.2059 - val_accuracy: 0.9260\n",
      "Epoch 145/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3630 - accuracy: 0.8746 - val_loss: 0.1909 - val_accuracy: 0.9301\n",
      "Epoch 146/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3881 - accuracy: 0.8635 - val_loss: 0.2083 - val_accuracy: 0.9151\n",
      "Epoch 147/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4098 - accuracy: 0.8527 - val_loss: 0.2561 - val_accuracy: 0.9055\n",
      "Epoch 148/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3617 - accuracy: 0.8723 - val_loss: 0.1773 - val_accuracy: 0.9384\n",
      "Epoch 149/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3429 - accuracy: 0.8773 - val_loss: 0.2852 - val_accuracy: 0.8890\n",
      "Epoch 150/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3810 - accuracy: 0.8682 - val_loss: 0.2390 - val_accuracy: 0.9205\n",
      "Epoch 151/300\n",
      "107/107 [==============================] - 1s 12ms/step - loss: 0.3988 - accuracy: 0.8585 - val_loss: 0.2021 - val_accuracy: 0.9247\n",
      "Epoch 152/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3736 - accuracy: 0.8743 - val_loss: 0.2056 - val_accuracy: 0.9356\n",
      "Epoch 153/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3654 - accuracy: 0.8655 - val_loss: 0.2336 - val_accuracy: 0.9219\n",
      "Epoch 154/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3476 - accuracy: 0.8744 - val_loss: 0.1867 - val_accuracy: 0.9384\n",
      "Epoch 155/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3731 - accuracy: 0.8648 - val_loss: 0.2047 - val_accuracy: 0.9288\n",
      "Epoch 156/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3998 - accuracy: 0.8563 - val_loss: 0.2088 - val_accuracy: 0.9274\n",
      "Epoch 157/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3856 - accuracy: 0.8646 - val_loss: 0.1984 - val_accuracy: 0.9233\n",
      "Epoch 158/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3529 - accuracy: 0.8814 - val_loss: 0.1575 - val_accuracy: 0.9466\n",
      "Epoch 159/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.2908 - accuracy: 0.8922 - val_loss: 0.1846 - val_accuracy: 0.9329\n",
      "Epoch 160/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3149 - accuracy: 0.8865 - val_loss: 0.2148 - val_accuracy: 0.9301\n",
      "Epoch 161/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3428 - accuracy: 0.8804 - val_loss: 0.2154 - val_accuracy: 0.9192\n",
      "Epoch 162/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3869 - accuracy: 0.8586 - val_loss: 0.2691 - val_accuracy: 0.9041\n",
      "Epoch 163/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.4286 - accuracy: 0.8534 - val_loss: 0.2281 - val_accuracy: 0.9110\n",
      "Epoch 164/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3507 - accuracy: 0.8780 - val_loss: 0.2319 - val_accuracy: 0.9219\n",
      "Epoch 165/300\n",
      "107/107 [==============================] - 1s 13ms/step - loss: 0.3878 - accuracy: 0.8696 - val_loss: 0.2099 - val_accuracy: 0.9315\n",
      "Epoch 166/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3665 - accuracy: 0.8655 - val_loss: 0.2008 - val_accuracy: 0.9370\n",
      "Epoch 167/300\n",
      "107/107 [==============================] - 1s 14ms/step - loss: 0.3312 - accuracy: 0.8891 - val_loss: 0.1793 - val_accuracy: 0.9342\n",
      "Epoch 168/300\n",
      "  6/107 [>.............................] - ETA: 1s - loss: 0.4245 - accuracy: 0.8546"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4e91a5cb2a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "model_spec.fit(train_spec,y_train,epochs=300,validation_data=(test_spec,y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKXVN69kJl0d"
   },
   "source": [
    "This model also gives us similar performance. Let's use a model architecture that uses both LSTM and Conv and see if it performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR0zvlCtC9TY"
   },
   "source": [
    "### Conv + LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ76dViYCo1p"
   },
   "outputs": [],
   "source": [
    "input_layer=Input(shape=(train_spec.shape[1],train_spec.shape[2]))   # Input layer\n",
    "lstm_layer=LSTM(units=50,return_sequences=True)(input_layer)  # LSTM layer with 25 units\n",
    "average=GlobalAveragePooling1D()(lstm_layer) #Global average pool\n",
    "layer_1=Dense(32,activation='relu')(average) # Dense layer\n",
    "\n",
    "\n",
    "x=Conv1D(filters=24,kernel_size=5)(input_layer)\n",
    "x=MaxPooling1D(strides=2)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Conv1D(filters=48,kernel_size=5)(x)\n",
    "x=MaxPooling1D(strides=2)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=Conv1D(filters=48,kernel_size=5)(x)\n",
    "x=GlobalAveragePooling1D()(x)\n",
    "x=Dense(64,activation='relu')(x) # Dense layer\n",
    "\n",
    "\n",
    "x=Concatenate()([layer_1,x])\n",
    "x=Dense(64)(x)\n",
    "x=Dropout(0.3)(x)\n",
    "output=Dense(24,activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[input_layer],outputs=output)\n",
    "optimizer=tf.keras.optimizers.Adam()  # Optimizer\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4x6IMrrTdLiM",
    "outputId": "c8236e8b-36c5-4345-b6d8-8bc15a835d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           [(None, 64, 1115)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 60, 24)       133824      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 30, 24)       0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 30, 24)       0           max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 26, 48)       5808        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 13, 48)       0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 13, 48)       0           max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 64, 50)       233200      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 9, 48)        11568       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 50)           0           lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 48)           0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           1632        global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 64)           3136        global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           dense_22[0][0]                   \n",
      "                                                                 dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 64)           6208        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64)           0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 24)           1560        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 396,936\n",
      "Trainable params: 396,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0kenXP3dLgE",
    "outputId": "ea58639a-158c-44b0-aad6-56677fb6da35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "107/107 [==============================] - 4s 22ms/step - loss: 2.6507 - accuracy: 0.1816 - val_loss: 1.7868 - val_accuracy: 0.3644\n",
      "Epoch 2/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 2.1241 - accuracy: 0.2788 - val_loss: 1.4596 - val_accuracy: 0.5082\n",
      "Epoch 3/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.8381 - accuracy: 0.3802 - val_loss: 1.3912 - val_accuracy: 0.4712\n",
      "Epoch 4/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.8000 - accuracy: 0.4004 - val_loss: 1.1852 - val_accuracy: 0.6137\n",
      "Epoch 5/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.6084 - accuracy: 0.4723 - val_loss: 1.1163 - val_accuracy: 0.6493\n",
      "Epoch 6/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.5043 - accuracy: 0.5064 - val_loss: 1.0307 - val_accuracy: 0.6740\n",
      "Epoch 7/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.4009 - accuracy: 0.5173 - val_loss: 0.9267 - val_accuracy: 0.6986\n",
      "Epoch 8/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.3173 - accuracy: 0.5680 - val_loss: 0.8445 - val_accuracy: 0.7055\n",
      "Epoch 9/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.2391 - accuracy: 0.5949 - val_loss: 0.7523 - val_accuracy: 0.7493\n",
      "Epoch 10/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 1.1517 - accuracy: 0.6150 - val_loss: 0.7189 - val_accuracy: 0.7384\n",
      "Epoch 11/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 1.0886 - accuracy: 0.6437 - val_loss: 0.6410 - val_accuracy: 0.8055\n",
      "Epoch 12/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.9652 - accuracy: 0.6678 - val_loss: 0.5901 - val_accuracy: 0.7973\n",
      "Epoch 13/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.9475 - accuracy: 0.6812 - val_loss: 0.5436 - val_accuracy: 0.8137\n",
      "Epoch 14/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.9603 - accuracy: 0.6897 - val_loss: 0.5654 - val_accuracy: 0.8315\n",
      "Epoch 15/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.8903 - accuracy: 0.7074 - val_loss: 0.5522 - val_accuracy: 0.8301\n",
      "Epoch 16/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.8627 - accuracy: 0.7136 - val_loss: 0.5429 - val_accuracy: 0.8219\n",
      "Epoch 17/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.8326 - accuracy: 0.7343 - val_loss: 0.4422 - val_accuracy: 0.8712\n",
      "Epoch 18/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.7997 - accuracy: 0.7376 - val_loss: 0.3573 - val_accuracy: 0.8986\n",
      "Epoch 19/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.7156 - accuracy: 0.7606 - val_loss: 0.4400 - val_accuracy: 0.8616\n",
      "Epoch 20/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.6796 - accuracy: 0.7684 - val_loss: 0.4620 - val_accuracy: 0.8452\n",
      "Epoch 21/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.8311 - accuracy: 0.7446 - val_loss: 0.4521 - val_accuracy: 0.8370\n",
      "Epoch 22/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.6919 - accuracy: 0.7716 - val_loss: 0.4515 - val_accuracy: 0.8521\n",
      "Epoch 23/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.6962 - accuracy: 0.7575 - val_loss: 0.3469 - val_accuracy: 0.8959\n",
      "Epoch 24/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.6253 - accuracy: 0.8125 - val_loss: 0.3324 - val_accuracy: 0.8740\n",
      "Epoch 25/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.6429 - accuracy: 0.7834 - val_loss: 0.4398 - val_accuracy: 0.8575\n",
      "Epoch 26/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.5743 - accuracy: 0.8074 - val_loss: 0.3540 - val_accuracy: 0.8822\n",
      "Epoch 27/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.5950 - accuracy: 0.7961 - val_loss: 0.3603 - val_accuracy: 0.8781\n",
      "Epoch 28/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.5654 - accuracy: 0.8076 - val_loss: 0.3086 - val_accuracy: 0.9055\n",
      "Epoch 29/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.5417 - accuracy: 0.8159 - val_loss: 0.2691 - val_accuracy: 0.9219\n",
      "Epoch 30/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4531 - accuracy: 0.8508 - val_loss: 0.2195 - val_accuracy: 0.9301\n",
      "Epoch 31/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4787 - accuracy: 0.8389 - val_loss: 0.3377 - val_accuracy: 0.8904\n",
      "Epoch 32/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.5083 - accuracy: 0.8251 - val_loss: 0.2511 - val_accuracy: 0.9068\n",
      "Epoch 33/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4719 - accuracy: 0.8536 - val_loss: 0.2593 - val_accuracy: 0.9000\n",
      "Epoch 34/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4136 - accuracy: 0.8661 - val_loss: 0.2605 - val_accuracy: 0.9110\n",
      "Epoch 35/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.5078 - accuracy: 0.8396 - val_loss: 0.2749 - val_accuracy: 0.9082\n",
      "Epoch 36/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.4185 - accuracy: 0.8563 - val_loss: 0.2078 - val_accuracy: 0.9260\n",
      "Epoch 37/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.3753 - accuracy: 0.8761 - val_loss: 0.3246 - val_accuracy: 0.9014\n",
      "Epoch 38/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.5054 - accuracy: 0.8440 - val_loss: 0.3932 - val_accuracy: 0.8630\n",
      "Epoch 39/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4121 - accuracy: 0.8531 - val_loss: 0.2714 - val_accuracy: 0.9096\n",
      "Epoch 40/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.3839 - accuracy: 0.8734 - val_loss: 0.2282 - val_accuracy: 0.9274\n",
      "Epoch 41/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.3573 - accuracy: 0.8843 - val_loss: 0.2218 - val_accuracy: 0.9233\n",
      "Epoch 42/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4101 - accuracy: 0.8612 - val_loss: 0.2299 - val_accuracy: 0.9219\n",
      "Epoch 43/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.3294 - accuracy: 0.8904 - val_loss: 0.2439 - val_accuracy: 0.9151\n",
      "Epoch 44/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.3533 - accuracy: 0.8853 - val_loss: 0.1697 - val_accuracy: 0.9438\n",
      "Epoch 45/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.3078 - accuracy: 0.8998 - val_loss: 0.2642 - val_accuracy: 0.9041\n",
      "Epoch 46/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.4314 - accuracy: 0.8576 - val_loss: 0.1999 - val_accuracy: 0.9466\n",
      "Epoch 47/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.3440 - accuracy: 0.8887 - val_loss: 0.1498 - val_accuracy: 0.9562\n",
      "Epoch 48/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.3241 - accuracy: 0.8865 - val_loss: 0.4673 - val_accuracy: 0.8616\n",
      "Epoch 49/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.3523 - accuracy: 0.8880 - val_loss: 0.1805 - val_accuracy: 0.9425\n",
      "Epoch 50/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.3561 - accuracy: 0.8883 - val_loss: 0.1851 - val_accuracy: 0.9384\n",
      "Epoch 51/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2776 - accuracy: 0.9133 - val_loss: 0.2552 - val_accuracy: 0.9247\n",
      "Epoch 52/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2594 - accuracy: 0.9132 - val_loss: 0.2321 - val_accuracy: 0.9274\n",
      "Epoch 53/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.3501 - accuracy: 0.8807 - val_loss: 0.1403 - val_accuracy: 0.9493\n",
      "Epoch 54/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2978 - accuracy: 0.9008 - val_loss: 0.1490 - val_accuracy: 0.9589\n",
      "Epoch 55/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2467 - accuracy: 0.9154 - val_loss: 0.1476 - val_accuracy: 0.9562\n",
      "Epoch 56/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2999 - accuracy: 0.9136 - val_loss: 0.1357 - val_accuracy: 0.9562\n",
      "Epoch 57/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2192 - accuracy: 0.9274 - val_loss: 0.1752 - val_accuracy: 0.9411\n",
      "Epoch 58/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2345 - accuracy: 0.9187 - val_loss: 0.2451 - val_accuracy: 0.9247\n",
      "Epoch 59/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2460 - accuracy: 0.9199 - val_loss: 0.1897 - val_accuracy: 0.9452\n",
      "Epoch 60/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2611 - accuracy: 0.9144 - val_loss: 0.1506 - val_accuracy: 0.9493\n",
      "Epoch 61/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.2494 - accuracy: 0.9214 - val_loss: 0.1272 - val_accuracy: 0.9630\n",
      "Epoch 62/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1922 - accuracy: 0.9367 - val_loss: 0.1232 - val_accuracy: 0.9630\n",
      "Epoch 63/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2188 - accuracy: 0.9210 - val_loss: 0.1119 - val_accuracy: 0.9616\n",
      "Epoch 64/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1886 - accuracy: 0.9372 - val_loss: 0.1625 - val_accuracy: 0.9466\n",
      "Epoch 65/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2779 - accuracy: 0.9099 - val_loss: 0.1374 - val_accuracy: 0.9658\n",
      "Epoch 66/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2112 - accuracy: 0.9282 - val_loss: 0.1575 - val_accuracy: 0.9534\n",
      "Epoch 67/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2121 - accuracy: 0.9273 - val_loss: 0.1130 - val_accuracy: 0.9658\n",
      "Epoch 68/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1677 - accuracy: 0.9474 - val_loss: 0.1415 - val_accuracy: 0.9507\n",
      "Epoch 69/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1782 - accuracy: 0.9425 - val_loss: 0.1417 - val_accuracy: 0.9575\n",
      "Epoch 70/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1931 - accuracy: 0.9389 - val_loss: 0.1700 - val_accuracy: 0.9466\n",
      "Epoch 71/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1852 - accuracy: 0.9312 - val_loss: 0.1571 - val_accuracy: 0.9548\n",
      "Epoch 72/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1553 - accuracy: 0.9494 - val_loss: 0.1363 - val_accuracy: 0.9548\n",
      "Epoch 73/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1908 - accuracy: 0.9343 - val_loss: 0.2059 - val_accuracy: 0.9438\n",
      "Epoch 74/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2505 - accuracy: 0.9235 - val_loss: 0.1474 - val_accuracy: 0.9616\n",
      "Epoch 75/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1752 - accuracy: 0.9403 - val_loss: 0.1036 - val_accuracy: 0.9767\n",
      "Epoch 76/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1197 - accuracy: 0.9615 - val_loss: 0.1149 - val_accuracy: 0.9658\n",
      "Epoch 77/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1506 - accuracy: 0.9495 - val_loss: 0.1152 - val_accuracy: 0.9658\n",
      "Epoch 78/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1854 - accuracy: 0.9416 - val_loss: 0.1581 - val_accuracy: 0.9575\n",
      "Epoch 79/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2105 - accuracy: 0.9330 - val_loss: 0.1797 - val_accuracy: 0.9466\n",
      "Epoch 80/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2005 - accuracy: 0.9377 - val_loss: 0.1687 - val_accuracy: 0.9534\n",
      "Epoch 81/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1486 - accuracy: 0.9523 - val_loss: 0.1801 - val_accuracy: 0.9548\n",
      "Epoch 82/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1617 - accuracy: 0.9479 - val_loss: 0.1452 - val_accuracy: 0.9493\n",
      "Epoch 83/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1172 - accuracy: 0.9612 - val_loss: 0.1475 - val_accuracy: 0.9616\n",
      "Epoch 84/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1818 - accuracy: 0.9386 - val_loss: 0.1299 - val_accuracy: 0.9671\n",
      "Epoch 85/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1355 - accuracy: 0.9544 - val_loss: 0.1012 - val_accuracy: 0.9753\n",
      "Epoch 86/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1405 - accuracy: 0.9565 - val_loss: 0.1608 - val_accuracy: 0.9548\n",
      "Epoch 87/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2321 - accuracy: 0.9287 - val_loss: 0.1232 - val_accuracy: 0.9630\n",
      "Epoch 88/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1071 - accuracy: 0.9676 - val_loss: 0.1326 - val_accuracy: 0.9699\n",
      "Epoch 89/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1336 - accuracy: 0.9544 - val_loss: 0.1177 - val_accuracy: 0.9644\n",
      "Epoch 90/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1321 - accuracy: 0.9594 - val_loss: 0.1494 - val_accuracy: 0.9603\n",
      "Epoch 91/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1001 - accuracy: 0.9675 - val_loss: 0.1096 - val_accuracy: 0.9616\n",
      "Epoch 92/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1754 - accuracy: 0.9438 - val_loss: 0.2223 - val_accuracy: 0.9329\n",
      "Epoch 93/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2554 - accuracy: 0.9249 - val_loss: 0.2879 - val_accuracy: 0.9274\n",
      "Epoch 94/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.3172 - accuracy: 0.8975 - val_loss: 0.1702 - val_accuracy: 0.9521\n",
      "Epoch 95/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2223 - accuracy: 0.9397 - val_loss: 0.2603 - val_accuracy: 0.9233\n",
      "Epoch 96/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1970 - accuracy: 0.9345 - val_loss: 0.2596 - val_accuracy: 0.9123\n",
      "Epoch 97/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2397 - accuracy: 0.9249 - val_loss: 0.1311 - val_accuracy: 0.9562\n",
      "Epoch 98/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1434 - accuracy: 0.9518 - val_loss: 0.1651 - val_accuracy: 0.9548\n",
      "Epoch 99/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1460 - accuracy: 0.9480 - val_loss: 0.1212 - val_accuracy: 0.9616\n",
      "Epoch 100/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0775 - accuracy: 0.9712 - val_loss: 0.1167 - val_accuracy: 0.9699\n",
      "Epoch 101/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1665 - accuracy: 0.9477 - val_loss: 0.1762 - val_accuracy: 0.9521\n",
      "Epoch 102/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1138 - accuracy: 0.9614 - val_loss: 0.1591 - val_accuracy: 0.9548\n",
      "Epoch 103/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1354 - accuracy: 0.9543 - val_loss: 0.1454 - val_accuracy: 0.9671\n",
      "Epoch 104/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1867 - accuracy: 0.9383 - val_loss: 0.1256 - val_accuracy: 0.9671\n",
      "Epoch 105/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0992 - accuracy: 0.9672 - val_loss: 0.1019 - val_accuracy: 0.9699\n",
      "Epoch 106/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0800 - accuracy: 0.9743 - val_loss: 0.1084 - val_accuracy: 0.9726\n",
      "Epoch 107/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1025 - accuracy: 0.9674 - val_loss: 0.1915 - val_accuracy: 0.9493\n",
      "Epoch 108/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1244 - accuracy: 0.9575 - val_loss: 0.2069 - val_accuracy: 0.9493\n",
      "Epoch 109/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0852 - accuracy: 0.9680 - val_loss: 0.1302 - val_accuracy: 0.9630\n",
      "Epoch 110/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0886 - accuracy: 0.9727 - val_loss: 0.1089 - val_accuracy: 0.9685\n",
      "Epoch 111/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0775 - accuracy: 0.9768 - val_loss: 0.1450 - val_accuracy: 0.9671\n",
      "Epoch 112/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0971 - accuracy: 0.9678 - val_loss: 0.1798 - val_accuracy: 0.9548\n",
      "Epoch 113/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1448 - accuracy: 0.9535 - val_loss: 0.1272 - val_accuracy: 0.9575\n",
      "Epoch 114/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1156 - accuracy: 0.9651 - val_loss: 0.1770 - val_accuracy: 0.9589\n",
      "Epoch 115/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1714 - accuracy: 0.9472 - val_loss: 0.1580 - val_accuracy: 0.9562\n",
      "Epoch 116/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0826 - accuracy: 0.9759 - val_loss: 0.1591 - val_accuracy: 0.9562\n",
      "Epoch 117/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0789 - accuracy: 0.9788 - val_loss: 0.1250 - val_accuracy: 0.9767\n",
      "Epoch 118/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1489 - accuracy: 0.9513 - val_loss: 0.1787 - val_accuracy: 0.9548\n",
      "Epoch 119/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1774 - accuracy: 0.9404 - val_loss: 0.1706 - val_accuracy: 0.9603\n",
      "Epoch 120/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0735 - accuracy: 0.9786 - val_loss: 0.1497 - val_accuracy: 0.9685\n",
      "Epoch 121/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0548 - accuracy: 0.9848 - val_loss: 0.1341 - val_accuracy: 0.9603\n",
      "Epoch 122/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0633 - accuracy: 0.9784 - val_loss: 0.1871 - val_accuracy: 0.9507\n",
      "Epoch 123/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0769 - accuracy: 0.9745 - val_loss: 0.1404 - val_accuracy: 0.9630\n",
      "Epoch 124/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0523 - accuracy: 0.9806 - val_loss: 0.1377 - val_accuracy: 0.9616\n",
      "Epoch 125/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0472 - accuracy: 0.9836 - val_loss: 0.1961 - val_accuracy: 0.9493\n",
      "Epoch 126/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0681 - accuracy: 0.9745 - val_loss: 0.1636 - val_accuracy: 0.9534\n",
      "Epoch 127/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0807 - accuracy: 0.9757 - val_loss: 0.1682 - val_accuracy: 0.9507\n",
      "Epoch 128/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.2111 - accuracy: 0.9387 - val_loss: 0.1472 - val_accuracy: 0.9562\n",
      "Epoch 129/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2175 - accuracy: 0.9317 - val_loss: 0.1423 - val_accuracy: 0.9685\n",
      "Epoch 130/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0873 - accuracy: 0.9733 - val_loss: 0.1496 - val_accuracy: 0.9589\n",
      "Epoch 131/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0816 - accuracy: 0.9743 - val_loss: 0.1689 - val_accuracy: 0.9575\n",
      "Epoch 132/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0929 - accuracy: 0.9682 - val_loss: 0.1613 - val_accuracy: 0.9644\n",
      "Epoch 133/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0530 - accuracy: 0.9809 - val_loss: 0.1339 - val_accuracy: 0.9630\n",
      "Epoch 134/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0568 - accuracy: 0.9824 - val_loss: 0.2162 - val_accuracy: 0.9438\n",
      "Epoch 135/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1533 - accuracy: 0.9507 - val_loss: 0.1536 - val_accuracy: 0.9630\n",
      "Epoch 136/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1252 - accuracy: 0.9633 - val_loss: 0.1393 - val_accuracy: 0.9671\n",
      "Epoch 137/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0510 - accuracy: 0.9831 - val_loss: 0.1291 - val_accuracy: 0.9685\n",
      "Epoch 138/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0453 - accuracy: 0.9835 - val_loss: 0.2668 - val_accuracy: 0.9493\n",
      "Epoch 139/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1311 - accuracy: 0.9592 - val_loss: 0.1897 - val_accuracy: 0.9452\n",
      "Epoch 140/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1148 - accuracy: 0.9656 - val_loss: 0.1019 - val_accuracy: 0.9753\n",
      "Epoch 141/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0265 - accuracy: 0.9943 - val_loss: 0.1276 - val_accuracy: 0.9767\n",
      "Epoch 142/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0257 - accuracy: 0.9937 - val_loss: 0.2071 - val_accuracy: 0.9438\n",
      "Epoch 143/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1485 - accuracy: 0.9504 - val_loss: 0.1314 - val_accuracy: 0.9658\n",
      "Epoch 144/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0758 - accuracy: 0.9749 - val_loss: 0.1953 - val_accuracy: 0.9616\n",
      "Epoch 145/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1146 - accuracy: 0.9656 - val_loss: 0.2022 - val_accuracy: 0.9562\n",
      "Epoch 146/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1357 - accuracy: 0.9619 - val_loss: 0.1981 - val_accuracy: 0.9534\n",
      "Epoch 147/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0956 - accuracy: 0.9717 - val_loss: 0.1687 - val_accuracy: 0.9589\n",
      "Epoch 148/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1218 - accuracy: 0.9647 - val_loss: 0.2504 - val_accuracy: 0.9507\n",
      "Epoch 149/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0871 - accuracy: 0.9731 - val_loss: 0.1536 - val_accuracy: 0.9603\n",
      "Epoch 150/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0729 - accuracy: 0.9734 - val_loss: 0.2172 - val_accuracy: 0.9548\n",
      "Epoch 151/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0993 - accuracy: 0.9670 - val_loss: 0.1487 - val_accuracy: 0.9699\n",
      "Epoch 152/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0739 - accuracy: 0.9780 - val_loss: 0.2525 - val_accuracy: 0.9479\n",
      "Epoch 153/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1408 - accuracy: 0.9588 - val_loss: 0.1345 - val_accuracy: 0.9712\n",
      "Epoch 154/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1894 - accuracy: 0.9488 - val_loss: 0.1145 - val_accuracy: 0.9671\n",
      "Epoch 155/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0977 - accuracy: 0.9662 - val_loss: 0.1095 - val_accuracy: 0.9712\n",
      "Epoch 156/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0229 - accuracy: 0.9931 - val_loss: 0.1453 - val_accuracy: 0.9644\n",
      "Epoch 157/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0239 - accuracy: 0.9916 - val_loss: 0.1541 - val_accuracy: 0.9671\n",
      "Epoch 158/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0503 - accuracy: 0.9827 - val_loss: 0.2721 - val_accuracy: 0.9301\n",
      "Epoch 159/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1559 - accuracy: 0.9510 - val_loss: 0.2430 - val_accuracy: 0.9479\n",
      "Epoch 160/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1329 - accuracy: 0.9574 - val_loss: 0.1860 - val_accuracy: 0.9493\n",
      "Epoch 161/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1231 - accuracy: 0.9594 - val_loss: 0.1571 - val_accuracy: 0.9603\n",
      "Epoch 162/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0488 - accuracy: 0.9839 - val_loss: 0.1521 - val_accuracy: 0.9644\n",
      "Epoch 163/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0904 - accuracy: 0.9703 - val_loss: 0.1394 - val_accuracy: 0.9671\n",
      "Epoch 164/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0267 - accuracy: 0.9931 - val_loss: 0.1373 - val_accuracy: 0.9753\n",
      "Epoch 165/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0528 - accuracy: 0.9805 - val_loss: 0.1302 - val_accuracy: 0.9644\n",
      "Epoch 166/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0866 - accuracy: 0.9719 - val_loss: 0.1661 - val_accuracy: 0.9630\n",
      "Epoch 167/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0830 - accuracy: 0.9792 - val_loss: 0.2148 - val_accuracy: 0.9521\n",
      "Epoch 168/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0595 - accuracy: 0.9798 - val_loss: 0.1635 - val_accuracy: 0.9699\n",
      "Epoch 169/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0395 - accuracy: 0.9834 - val_loss: 0.1632 - val_accuracy: 0.9699\n",
      "Epoch 170/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1137 - accuracy: 0.9718 - val_loss: 0.1419 - val_accuracy: 0.9671\n",
      "Epoch 171/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0295 - accuracy: 0.9890 - val_loss: 0.2020 - val_accuracy: 0.9548\n",
      "Epoch 172/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1266 - accuracy: 0.9611 - val_loss: 0.1626 - val_accuracy: 0.9562\n",
      "Epoch 173/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0846 - accuracy: 0.9774 - val_loss: 0.1438 - val_accuracy: 0.9616\n",
      "Epoch 174/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1838 - accuracy: 0.9497 - val_loss: 0.1476 - val_accuracy: 0.9589\n",
      "Epoch 175/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0587 - accuracy: 0.9830 - val_loss: 0.1376 - val_accuracy: 0.9740\n",
      "Epoch 176/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.0982 - val_accuracy: 0.9795\n",
      "Epoch 177/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0327 - accuracy: 0.9903 - val_loss: 0.1322 - val_accuracy: 0.9767\n",
      "Epoch 178/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0353 - accuracy: 0.9864 - val_loss: 0.1264 - val_accuracy: 0.9685\n",
      "Epoch 179/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0398 - accuracy: 0.9872 - val_loss: 0.1591 - val_accuracy: 0.9575\n",
      "Epoch 180/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1582 - accuracy: 0.9570 - val_loss: 0.2194 - val_accuracy: 0.9507\n",
      "Epoch 181/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1282 - accuracy: 0.9609 - val_loss: 0.1287 - val_accuracy: 0.9699\n",
      "Epoch 182/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0342 - accuracy: 0.9864 - val_loss: 0.1932 - val_accuracy: 0.9589\n",
      "Epoch 183/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0696 - accuracy: 0.9834 - val_loss: 0.1697 - val_accuracy: 0.9603\n",
      "Epoch 184/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0629 - accuracy: 0.9712 - val_loss: 0.1858 - val_accuracy: 0.9575\n",
      "Epoch 185/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0516 - accuracy: 0.9829 - val_loss: 0.1377 - val_accuracy: 0.9685\n",
      "Epoch 186/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0841 - accuracy: 0.9704 - val_loss: 0.1405 - val_accuracy: 0.9699\n",
      "Epoch 187/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1467 - accuracy: 0.9588 - val_loss: 0.1811 - val_accuracy: 0.9589\n",
      "Epoch 188/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0486 - accuracy: 0.9806 - val_loss: 0.1463 - val_accuracy: 0.9671\n",
      "Epoch 189/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0226 - accuracy: 0.9936 - val_loss: 0.1529 - val_accuracy: 0.9712\n",
      "Epoch 190/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.1510 - val_accuracy: 0.9644\n",
      "Epoch 191/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 0.1894 - val_accuracy: 0.9699\n",
      "Epoch 192/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0948 - accuracy: 0.9687 - val_loss: 0.3055 - val_accuracy: 0.9329\n",
      "Epoch 193/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0589 - accuracy: 0.9787 - val_loss: 0.3220 - val_accuracy: 0.9411\n",
      "Epoch 194/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0629 - accuracy: 0.9861 - val_loss: 0.2472 - val_accuracy: 0.9521\n",
      "Epoch 195/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0848 - accuracy: 0.9727 - val_loss: 0.1641 - val_accuracy: 0.9740\n",
      "Epoch 196/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0909 - accuracy: 0.9719 - val_loss: 0.2773 - val_accuracy: 0.9521\n",
      "Epoch 197/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1555 - accuracy: 0.9551 - val_loss: 0.2792 - val_accuracy: 0.9397\n",
      "Epoch 198/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0678 - accuracy: 0.9774 - val_loss: 0.1408 - val_accuracy: 0.9726\n",
      "Epoch 199/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0374 - accuracy: 0.9874 - val_loss: 0.1498 - val_accuracy: 0.9685\n",
      "Epoch 200/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0243 - accuracy: 0.9919 - val_loss: 0.1922 - val_accuracy: 0.9671\n",
      "Epoch 201/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0383 - accuracy: 0.9850 - val_loss: 0.2448 - val_accuracy: 0.9521\n",
      "Epoch 202/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0834 - accuracy: 0.9719 - val_loss: 0.1699 - val_accuracy: 0.9562\n",
      "Epoch 203/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0551 - accuracy: 0.9800 - val_loss: 0.2374 - val_accuracy: 0.9479\n",
      "Epoch 204/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1103 - accuracy: 0.9656 - val_loss: 0.2130 - val_accuracy: 0.9507\n",
      "Epoch 205/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.1100 - accuracy: 0.9691 - val_loss: 0.1550 - val_accuracy: 0.9726\n",
      "Epoch 206/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0475 - accuracy: 0.9823 - val_loss: 0.1360 - val_accuracy: 0.9699\n",
      "Epoch 207/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0587 - accuracy: 0.9796 - val_loss: 0.1778 - val_accuracy: 0.9603\n",
      "Epoch 208/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0466 - accuracy: 0.9885 - val_loss: 0.1593 - val_accuracy: 0.9644\n",
      "Epoch 209/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1626 - accuracy: 0.9549 - val_loss: 0.2496 - val_accuracy: 0.9425\n",
      "Epoch 210/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0748 - accuracy: 0.9778 - val_loss: 0.2324 - val_accuracy: 0.9603\n",
      "Epoch 211/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0486 - accuracy: 0.9869 - val_loss: 0.1346 - val_accuracy: 0.9644\n",
      "Epoch 212/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0365 - accuracy: 0.9876 - val_loss: 0.1246 - val_accuracy: 0.9767\n",
      "Epoch 213/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0111 - accuracy: 0.9982 - val_loss: 0.1812 - val_accuracy: 0.9685\n",
      "Epoch 214/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0260 - accuracy: 0.9904 - val_loss: 0.1853 - val_accuracy: 0.9548\n",
      "Epoch 215/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0826 - accuracy: 0.9722 - val_loss: 0.1701 - val_accuracy: 0.9712\n",
      "Epoch 216/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0802 - accuracy: 0.9786 - val_loss: 0.2388 - val_accuracy: 0.9562\n",
      "Epoch 217/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1019 - accuracy: 0.9723 - val_loss: 0.1767 - val_accuracy: 0.9603\n",
      "Epoch 218/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0908 - accuracy: 0.9682 - val_loss: 0.1237 - val_accuracy: 0.9699\n",
      "Epoch 219/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0366 - accuracy: 0.9858 - val_loss: 0.1987 - val_accuracy: 0.9493\n",
      "Epoch 220/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0649 - accuracy: 0.9753 - val_loss: 0.1910 - val_accuracy: 0.9644\n",
      "Epoch 221/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0588 - accuracy: 0.9813 - val_loss: 0.1654 - val_accuracy: 0.9603\n",
      "Epoch 222/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0897 - accuracy: 0.9726 - val_loss: 0.1650 - val_accuracy: 0.9589\n",
      "Epoch 223/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0634 - accuracy: 0.9784 - val_loss: 0.1636 - val_accuracy: 0.9671\n",
      "Epoch 224/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0853 - accuracy: 0.9699 - val_loss: 0.1548 - val_accuracy: 0.9699\n",
      "Epoch 225/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0829 - accuracy: 0.9747 - val_loss: 0.2080 - val_accuracy: 0.9548\n",
      "Epoch 226/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0586 - accuracy: 0.9817 - val_loss: 0.1370 - val_accuracy: 0.9726\n",
      "Epoch 227/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0226 - accuracy: 0.9904 - val_loss: 0.1996 - val_accuracy: 0.9685\n",
      "Epoch 228/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0167 - accuracy: 0.9963 - val_loss: 0.1088 - val_accuracy: 0.9781\n",
      "Epoch 229/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0091 - accuracy: 0.9984 - val_loss: 0.1651 - val_accuracy: 0.9644\n",
      "Epoch 230/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0383 - accuracy: 0.9893 - val_loss: 0.3057 - val_accuracy: 0.9329\n",
      "Epoch 231/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.3710 - accuracy: 0.9135 - val_loss: 0.2325 - val_accuracy: 0.9356\n",
      "Epoch 232/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1508 - accuracy: 0.9557 - val_loss: 0.1586 - val_accuracy: 0.9658\n",
      "Epoch 233/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0445 - accuracy: 0.9855 - val_loss: 0.1342 - val_accuracy: 0.9767\n",
      "Epoch 234/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0257 - accuracy: 0.9927 - val_loss: 0.1274 - val_accuracy: 0.9699\n",
      "Epoch 235/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0135 - accuracy: 0.9959 - val_loss: 0.1410 - val_accuracy: 0.9685\n",
      "Epoch 236/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0203 - accuracy: 0.9948 - val_loss: 0.2156 - val_accuracy: 0.9658\n",
      "Epoch 237/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0589 - accuracy: 0.9841 - val_loss: 0.2429 - val_accuracy: 0.9521\n",
      "Epoch 238/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0570 - accuracy: 0.9783 - val_loss: 0.3067 - val_accuracy: 0.9315\n",
      "Epoch 239/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1815 - accuracy: 0.9539 - val_loss: 0.2012 - val_accuracy: 0.9507\n",
      "Epoch 240/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0602 - accuracy: 0.9817 - val_loss: 0.1415 - val_accuracy: 0.9699\n",
      "Epoch 241/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0611 - accuracy: 0.9831 - val_loss: 0.2591 - val_accuracy: 0.9507\n",
      "Epoch 242/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0596 - accuracy: 0.9797 - val_loss: 0.2525 - val_accuracy: 0.9479\n",
      "Epoch 243/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0633 - accuracy: 0.9767 - val_loss: 0.1409 - val_accuracy: 0.9753\n",
      "Epoch 244/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0123 - accuracy: 0.9979 - val_loss: 0.1257 - val_accuracy: 0.9726\n",
      "Epoch 245/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.1156 - val_accuracy: 0.9822\n",
      "Epoch 246/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.1297 - val_accuracy: 0.9767\n",
      "Epoch 247/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0588 - accuracy: 0.9850 - val_loss: 0.1402 - val_accuracy: 0.9781\n",
      "Epoch 248/300\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.0226 - accuracy: 0.9947 - val_loss: 0.2469 - val_accuracy: 0.9479\n",
      "Epoch 249/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0752 - accuracy: 0.9784 - val_loss: 0.2056 - val_accuracy: 0.9644\n",
      "Epoch 250/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0605 - accuracy: 0.9842 - val_loss: 0.2157 - val_accuracy: 0.9603\n",
      "Epoch 251/300\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.1091 - accuracy: 0.9683 - val_loss: 0.2552 - val_accuracy: 0.9425\n",
      "Epoch 252/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1563 - accuracy: 0.9493 - val_loss: 0.2503 - val_accuracy: 0.9507\n",
      "Epoch 253/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0704 - accuracy: 0.9787 - val_loss: 0.2163 - val_accuracy: 0.9575\n",
      "Epoch 254/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0316 - accuracy: 0.9887 - val_loss: 0.1982 - val_accuracy: 0.9644\n",
      "Epoch 255/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0519 - accuracy: 0.9865 - val_loss: 0.1416 - val_accuracy: 0.9753\n",
      "Epoch 256/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0231 - accuracy: 0.9939 - val_loss: 0.1290 - val_accuracy: 0.9767\n",
      "Epoch 257/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0723 - accuracy: 0.9822 - val_loss: 0.1891 - val_accuracy: 0.9548\n",
      "Epoch 258/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0560 - accuracy: 0.9818 - val_loss: 0.2288 - val_accuracy: 0.9548\n",
      "Epoch 259/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0515 - accuracy: 0.9876 - val_loss: 0.1279 - val_accuracy: 0.9685\n",
      "Epoch 260/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0882 - accuracy: 0.9688 - val_loss: 0.1878 - val_accuracy: 0.9630\n",
      "Epoch 261/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0386 - accuracy: 0.9876 - val_loss: 0.1790 - val_accuracy: 0.9603\n",
      "Epoch 262/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0480 - accuracy: 0.9853 - val_loss: 0.2284 - val_accuracy: 0.9575\n",
      "Epoch 263/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0603 - accuracy: 0.9806 - val_loss: 0.2071 - val_accuracy: 0.9726\n",
      "Epoch 264/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0171 - accuracy: 0.9950 - val_loss: 0.1490 - val_accuracy: 0.9712\n",
      "Epoch 265/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.1606 - val_accuracy: 0.9630\n",
      "Epoch 266/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0293 - accuracy: 0.9920 - val_loss: 0.2488 - val_accuracy: 0.9466\n",
      "Epoch 267/300\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.2302 - accuracy: 0.9345 - val_loss: 0.2583 - val_accuracy: 0.9507\n",
      "Epoch 268/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1839 - accuracy: 0.9495 - val_loss: 0.1775 - val_accuracy: 0.9589\n",
      "Epoch 269/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1089 - accuracy: 0.9736 - val_loss: 0.1517 - val_accuracy: 0.9726\n",
      "Epoch 270/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0717 - accuracy: 0.9762 - val_loss: 0.1829 - val_accuracy: 0.9616\n",
      "Epoch 271/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0150 - accuracy: 0.9960 - val_loss: 0.1390 - val_accuracy: 0.9795\n",
      "Epoch 272/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0066 - accuracy: 0.9996 - val_loss: 0.1270 - val_accuracy: 0.9822\n",
      "Epoch 273/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.1588 - val_accuracy: 0.9712\n",
      "Epoch 274/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9822\n",
      "Epoch 275/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0201 - accuracy: 0.9952 - val_loss: 0.1390 - val_accuracy: 0.9685\n",
      "Epoch 276/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.1047 - val_accuracy: 0.9740\n",
      "Epoch 277/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.1033 - val_accuracy: 0.9822\n",
      "Epoch 278/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.1190 - val_accuracy: 0.9767\n",
      "Epoch 279/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0214 - accuracy: 0.9926 - val_loss: 0.2499 - val_accuracy: 0.9425\n",
      "Epoch 280/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0917 - accuracy: 0.9745 - val_loss: 0.3947 - val_accuracy: 0.9247\n",
      "Epoch 281/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1272 - accuracy: 0.9599 - val_loss: 0.1696 - val_accuracy: 0.9644\n",
      "Epoch 282/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0452 - accuracy: 0.9833 - val_loss: 0.2039 - val_accuracy: 0.9575\n",
      "Epoch 283/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0347 - accuracy: 0.9872 - val_loss: 0.1575 - val_accuracy: 0.9685\n",
      "Epoch 284/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.1547 - val_accuracy: 0.9726\n",
      "Epoch 285/300\n",
      "107/107 [==============================] - 2s 18ms/step - loss: 0.0068 - accuracy: 0.9992 - val_loss: 0.1314 - val_accuracy: 0.9699\n",
      "Epoch 286/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.1859 - val_accuracy: 0.9603\n",
      "Epoch 287/300\n",
      "107/107 [==============================] - 2s 15ms/step - loss: 0.0421 - accuracy: 0.9895 - val_loss: 0.3726 - val_accuracy: 0.9219\n",
      "Epoch 288/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.1800 - accuracy: 0.9531 - val_loss: 0.4158 - val_accuracy: 0.9205\n",
      "Epoch 289/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.2287 - accuracy: 0.9385 - val_loss: 0.1739 - val_accuracy: 0.9493\n",
      "Epoch 290/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0850 - accuracy: 0.9730 - val_loss: 0.3706 - val_accuracy: 0.9356\n",
      "Epoch 291/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.1095 - accuracy: 0.9633 - val_loss: 0.1882 - val_accuracy: 0.9575\n",
      "Epoch 292/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 0.1895 - val_accuracy: 0.9658\n",
      "Epoch 293/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0103 - accuracy: 0.9975 - val_loss: 0.1706 - val_accuracy: 0.9589\n",
      "Epoch 294/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.1517 - val_accuracy: 0.9685\n",
      "Epoch 295/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0248 - accuracy: 0.9933 - val_loss: 0.1646 - val_accuracy: 0.9671\n",
      "Epoch 296/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.1790 - val_accuracy: 0.9589\n",
      "Epoch 297/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.2014 - val_accuracy: 0.9712\n",
      "Epoch 298/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.2210 - val_accuracy: 0.9658\n",
      "Epoch 299/300\n",
      "107/107 [==============================] - 2s 16ms/step - loss: 0.0128 - accuracy: 0.9972 - val_loss: 0.1768 - val_accuracy: 0.9644\n",
      "Epoch 300/300\n",
      "107/107 [==============================] - 2s 17ms/step - loss: 0.0330 - accuracy: 0.9878 - val_loss: 0.3659 - val_accuracy: 0.9219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f355c347710>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "model.fit(train_spec,y_train,epochs=300,validation_data=(test_spec,y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_StAFQCJ6dl"
   },
   "source": [
    "This model's performance is slightly better than previous two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lbLfYJF3w7D"
   },
   "source": [
    "Let's check our model's performance using using label_ranking_average_precision_score to evaluate submissions.\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "Or in easy language, we can define it as-     \n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBE-P1VfZnPz"
   },
   "outputs": [],
   "source": [
    "# Converting y_label to one hot encoded\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "true=enc.fit_transform(y_test)\n",
    "true=true.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3Ibb5MUEgkX",
    "outputId": "d2ce0bac-13a8-41e4-d46f-b183b054bb28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9638850837138507"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making prediction on our test data and checking label_ranking_average_precision_score\n",
    "pred=[model_spec.predict(i.reshape(1,64,1115))[0] for i in test_spec]\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "label_ranking_average_precision_score(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHQoBQJO4fd5"
   },
   "source": [
    "This is good, our model is performing very well !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_8MGMLHmjrv"
   },
   "source": [
    "## Summary of our work till now\n",
    "1. First we did a detailed EDA of our data and found some useful information which we later used during model preprocessing and modelling.\n",
    "2. Later, to preprocess our data we loaded the data using Librosa using sampling rate of 48000.\n",
    "3. We split our data into train and test(70:30). \n",
    "4. We added 0.2 seconds to left and right of the given timeframe and did data augmentation to make our model more generalised and robust.(Augmentation was only applied to training data).\n",
    "5. We converted our data from time domain to frequency domain by converting it to spectrograms.\n",
    "6. We normalised our data.\n",
    "7. We tried different neural network architecture to check which performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndR1SdN9ZnN7",
    "outputId": "5302f04f-4d0c-4434-ab75-79d4245f9299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+----------+\n",
      "|       Model       | Multi class logloss | Accuracy |\n",
      "+-------------------+---------------------+----------+\n",
      "| Convolution Model |         0.21        |    95    |\n",
      "|     LSTM Model    |         0.17        |    93    |\n",
      "| Conv + LSTM Model |         0.15        |    97    |\n",
      "+-------------------+---------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x = PrettyTable([\"Model\", \"Multi class logloss\",\"Accuracy\"])\n",
    "row1= ['Convolution Model',0.21,95]\n",
    "row2 = ['LSTM Model',0.17,93]\n",
    "row3 = ['Conv + LSTM Model',0.15,97]\n",
    "x.add_row(row1)\n",
    "x.add_row(row2)\n",
    "x.add_row(row3)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juedb39_qHLw"
   },
   "source": [
    "First, we tried with Convolution based neural network which performed quite good.<br>\n",
    "LSTM based network gave us better logloss but accuracy decreased slightly.<br>\n",
    "LSTM + Conv network performed best of them all and logloss and accuracy both got better than our previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5SMwlqTaNvP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1ujAh0QaNs5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJNsR_2QaNqQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessing and Modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
